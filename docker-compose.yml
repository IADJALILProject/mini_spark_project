version: '3.8'

services:

  ############################
  ##     INIT AIRFLOW      ##
  ############################

  airflow-init:
    image: apache/airflow:2.7.2
    container_name: airflow-init
    entrypoint: >
      bash -c "
        airflow db migrate &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
      "
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=bYfYpWfpOjwYx4ZFlPNqM7SAXkuwTICvOKaK1t5uvz0=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/db/airflow.db
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/db:/opt/airflow/db
    restart: "no"

  ############################
  ##      AIRFLOW WEB       ##
  ############################

  airflow-webserver:
    image: apache/airflow:2.7.2
    container_name: airflow-webserver
    depends_on:
      - airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=bYfYpWfpOjwYx4ZFlPNqM7SAXkuwTICvOKaK1t5uvz0=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/db/airflow.db
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/db:/opt/airflow/db
    ports:
      - "8080:8080"
    command: bash -c "airflow webserver"

  airflow-scheduler:
    image: apache/airflow:2.7.2
    container_name: airflow-scheduler
    depends_on:
      - airflow-webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=bYfYpWfpOjwYx4ZFlPNqM7SAXkuwTICvOKaK1t5uvz0=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/db/airflow.db
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/db:/opt/airflow/db
    command: bash -c "airflow scheduler"

  ############################
  ##     SPARK CLUSTER      ##
  ############################

  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    ports:
      - "8081:8080"
    environment:
      - SPARK_MODE=master
    volumes:
      - ./spark_jobs:/opt/spark_jobs
      - ./dbfs:/dbfs

  spark-worker-1:
    image: bitnami/spark:latest
    container_name: spark-worker-1
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    volumes:
      - ./spark_jobs:/opt/spark_jobs
      - ./dbfs:/dbfs

  spark-worker-2:
    image: bitnami/spark:latest
    container_name: spark-worker-2
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    volumes:
      - ./spark_jobs:/opt/spark_jobs
      - ./dbfs:/dbfs

  ############################
  ##      SPARK APP         ##
  ############################

  spark-app:
    build:
      context: .
      dockerfile: docker/spark-app.Dockerfile
    container_name: spark-app
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    volumes:
      - ./spark_jobs:/opt/spark_jobs
      - ./dbfs:/dbfs
    ports:
      - "4040:4040"
    entrypoint: ["/opt/spark_jobs/entrypoint.sh"]
